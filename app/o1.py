# -*- coding: utf-8 -*-
"""sqlbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BIfciMlRVERePyrUe7bQbxmLvk20zfZB
"""

import os
import json
import torch
import numpy as np
from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments
from torch.utils.data import Dataset

# Define dataset directory structure
DATASET_PATH = "./"

# Function to load JSON data
def load_json_data(folder, category):
    file_path = os.path.join(DATASET_PATH, f"{folder}.json")
    with open(file_path, "r", encoding="utf-8") as f:
        return json.load(f)

# Load student training, testing, and validation data
train_data = load_json_data("traning", "student")
test_data = load_json_data("testing", "student")
val_data = load_json_data("validation", "student")

# Load keymap for keyword expansion
keymap = load_json_data("keymap", "map")

# Function to preprocess data
def preprocess_data(data):
    inputs, outputs = [], []
    for item in data:
        user_query = item["user_input"]
        sql_query = item["sql_query"]

        # Replace keymap words with variations for better generalization
        for key, synonyms in keymap.items():
            for synonym in synonyms:
                if synonym in user_query:
                    user_query = user_query.replace(synonym, key)

        inputs.append(user_query)
        outputs.append(sql_query)
    return inputs, outputs

# Preprocess datasets
train_inputs, train_outputs = preprocess_data(train_data)
test_inputs, test_outputs = preprocess_data(test_data)
val_inputs, val_outputs = preprocess_data(val_data)

# Load pre-trained T5 tokenizer
tokenizer = T5Tokenizer.from_pretrained("t5-small")

# Tokenization function
def tokenize_function(inputs, outputs):
    return tokenizer(inputs, padding="max_length", truncation=True, max_length=128, return_tensors="pt"), \
           tokenizer(outputs, padding="max_length", truncation=True, max_length=128, return_tensors="pt")

# Tokenize datasets
train_encodings, train_labels = tokenize_function(train_inputs, train_outputs)
test_encodings, test_labels = tokenize_function(test_inputs, test_outputs)
val_encodings, val_labels = tokenize_function(val_inputs, val_outputs)

# Create PyTorch Dataset class
class SQLDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __len__(self):
        return len(self.labels["input_ids"])

    def __getitem__(self, idx):
        return {
            "input_ids": torch.tensor(self.encodings["input_ids"][idx]),
            "attention_mask": torch.tensor(self.encodings["attention_mask"][idx]),
            "labels": torch.tensor(self.labels["input_ids"][idx])
        }

# Create dataset instances
train_dataset = SQLDataset(train_encodings, train_labels)
test_dataset = SQLDataset(test_encodings, test_labels)
val_dataset = SQLDataset(val_encodings, val_labels)

# Load pre-trained T5 model for text generation
model = T5ForConditionalGeneration.from_pretrained("t5-small")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Disable weights & biases
os.environ["WANDB_DISABLED"] = "true"

# Define training arguments
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=3e-4,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=25,
    weight_decay=0.01,
    load_best_model_at_end=True
)

# Define Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset
)

# Train the model
trainer.train()

# Evaluate model on test dataset
results = trainer.evaluate(test_dataset)
print("Test Loss:", results["eval_loss"])

# Function to generate SQL queries from user input
def generate_sql(user_query):
    model.eval()
    user_query = tokenizer(user_query, return_tensors="pt", padding="max_length", truncation=True, max_length=128)
    user_query = {key: val.to(device) for key, val in user_query.items()}

    with torch.no_grad():
        output_ids = model.generate(**user_query, max_length=128)

    sql_query = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    return sql_query

# Example testing
sample_input = "Show all students who scored above 80 in Mathematics"
generated_sql = generate_sql(sample_input)
print("Generated SQL:", generated_sql)

# Get the model as pth file
model.save_pretrained("./results/model")
torch.save(model.state_dict(), "model.pth")

"""I am transfer this **torch file** (model.pth) to a **tensorflow file** (o1.keras), i am doing this because my machine not able to handle torch file.

And later i re-transfer it in **tensorflow js file** or **onnx file** because i am use it in a node js enviroment for use publically.
"""

import torch
import tensorflow as tf
from transformers import T5ForConditionalGeneration, T5Tokenizer, TFAutoModelForSeq2SeqLM

# Load tokenizer
tokenizer = T5Tokenizer.from_pretrained("t5-small")

# Load PyTorch model.pth
model_path = "model.pth"
model = T5ForConditionalGeneration.from_pretrained("t5-small")
model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))
model.eval()

# Convert to TensorFlow model
tf_model = TFAutoModelForSeq2SeqLM.from_pretrained("t5-small", from_pt=True)

# Save as TensorFlow Keras model
tf_model.save("./o1.keras")
print("Model successfully saved as model.keras")

#!pip install tensorflowjs
import tensorflowjs as tfjs

from transformers import T5ForConditionalGeneration, T5Tokenizer

# Load the PyTorch model
model_path = "./model.pth"  # Update with your correct path
model = T5ForConditionalGeneration.from_pretrained("t5-small")

# Load state dict from model.pth
state_dict = torch.load(model_path, map_location=torch.device("cpu"))
model.load_state_dict(state_dict)

# Save in Hugging Face Transformers format
save_path = "./model_hf"
model.save_pretrained(save_path)
tokenizer = T5Tokenizer.from_pretrained("t5-small")
tokenizer.save_pretrained(save_path)

print("✅ Model successfully converted to Hugging Face format and saved in ./model_hf")

# Load model in Hugging Face format
model = TFT5ForConditionalGeneration.from_pretrained("./model_hf")
tokenizer = T5Tokenizer.from_pretrained("./model_hf")

# Save model in TensorFlow SavedModel format
tf_model_path = "./o1_tf"
model.save_pretrained(tf_model_path, saved_model=True)

print("✅ Model successfully converted to TensorFlow SavedModel format at ./o1_tf")

import torch
import onnx
from transformers import T5ForConditionalGeneration

# Load the T5 model (same variant as your trained model)
model = T5ForConditionalGeneration.from_pretrained("t5-small")

# Load the trained weights from the .pth file
model.load_state_dict(torch.load("./model.pth", map_location=torch.device("cpu")))

# Set model to evaluation mode
model.eval()

# Define input tensors
dummy_input_ids = torch.randint(0, 100, (1, 128))  # Adjust input size
dummy_decoder_input_ids = torch.randint(0, 100, (1, 1))  # Decoder input

# **Wrap the model for ONNX export**
class OnnxModel(torch.nn.Module):
    def __init__(self, model):
        super().__init__()
        self.model = model

    def forward(self, input_ids, decoder_input_ids):
        # Explicitly pass decoder_input_ids to the model's forward method
        return self.model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)

onnx_model = OnnxModel(model) # Create an instance of the wrapper class

# Convert to ONNX format
onnx_model_path = "./o1.onnx"
torch.onnx.export(
    onnx_model, # Export the wrapped model
    (dummy_input_ids, dummy_decoder_input_ids),  # Pass both inputs
    onnx_model_path,
    input_names=["input_ids", "decoder_input_ids"],
    output_names=["output"],
    dynamic_axes={
        "input_ids": {0: "batch_size", 1: "seq_length"},
        "decoder_input_ids": {0: "batch_size", 1: "seq_length"},
        "output": {0: "batch_size", 1: "seq_length"},
    },
    opset_version=11
)

print("✅ Model successfully converted to ONNX:", onnx_model_path)